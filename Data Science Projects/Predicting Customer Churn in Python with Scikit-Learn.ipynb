{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 ) EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telco.info()\n",
    "telco['Churn'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2850 No, 483 Yes (Churners)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .groupby() method is incredibly useful when you want to investigate specific columns of your dataset. Here, you're going to explore the 'Churn' column further to see if there are differences between churners and non-churners. A subset version of the telco DataFrame, consisting of the columns 'Churn', 'CustServ_Calls', and 'Vmail_Message' is available in your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(telco.groupby(['Churn']).std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHURN        CustServ_Calls       Vmail_Message\n",
    "\n",
    "Yes          1.16                 13.91\n",
    "No           1.85                 11.96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(telco.groupby(['Churn']).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When dealing with customer data, geographic regions may play an important part in determining whether a customer will cancel their service or not. You may have noticed that there is a 'State' column in the dataset. In this exercise, you'll group 'State' and 'Churn' to count the number of churners and non-churners by state. \n",
    "\n",
    "For example, if you wanted to group by x and aggregate by y, you could use .groupby() as follows:\n",
    "\n",
    "df.groupby('x')['y'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(telco.groupby('State')['Churn'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import matplotlib and seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Visualize the distribution of 'Eve_Mins'\n",
    "sns.distplot(telco['Day_Mins'])\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of these features ('Day_Mins','Eve_Mins','Night_Mins','Intl_Mins') appear to be well approximated by the normal distribution. If this were not the case, we would have to consider applying a feature transformation of some kind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import matplotlib and seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create the box plot, sym=\"\" removes the outliers, hue=\"Vmail_Plan\"  to visualize whether or not having a voice mail plan \n",
    "# affects the number of customer service calls or churn\n",
    "sns.boxplot(x = 'Churn',\n",
    "            y = 'CustServ_Calls',\n",
    "            data = telco\n",
    "            sym=\"\"\n",
    "            hue = \"Vmail_Plan\" )\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is preferable to have features like 'Churn' encoded as 0 and 1 instead of no and yes, so that you can then feed it into machine learning algorithms that only accept numeric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'no' with 0 and 'yes' with 1 in 'Vmail_Plan'\n",
    "telco['Vmail_Plan'] = telco['Vmail_Plan'].replace({'no': 0 , 'yes': 1})\n",
    "\n",
    "# Replace 'no' with 0 and 'yes' with 1 in 'Churn'\n",
    "telco['Churn'] = telco['Churn'].replace({'no': 0 , 'yes': 1})\n",
    "\n",
    "# Print the results to verify\n",
    "print(telco['Vmail_Plan'].head())\n",
    "print(telco['Churn'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'State' has many different labels like KS,OH,NJ etc. It can be less effective and can give damage to our model when we label it like  1,2,3,4,5... that model can see it like increasing value.\n",
    "\n",
    "One Hot Encoding is a matrix style like;\n",
    "\n",
    "KS        OH      NJ     FR\n",
    "0         1       0       0\n",
    "0         0       0       1\n",
    "\n",
    "\n",
    "Doing this manually would be quite tedious, especially when you have 50 states and over 3000 customers! Fortunately, pandas has a get_dummies() function which automatically applies one hot encoding over the selected feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Perform one hot encoding on 'State'\n",
    "telco_state = pd.get_dummies(telco['State'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telco['Intl_Calls'].describe\n",
    "telco['Night_Mins'].describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we look these features,there is big difference between scales(for ex mean of Night_Mins is 200 and Intl_Calls's mean is 4)\n",
    "So, we will scale these features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale telco using StandardScaler\n",
    "telco_scaled = StandardScaler().fit_transform(telco)\n",
    "\n",
    "# Add column names back for readability\n",
    "telco_scaled_df = pd.DataFrame(telco_scaled, columns=[\"Intl_Calls\", \"Night_Mins\"])\n",
    "\n",
    "# Print summary statistics\n",
    "print(telco_scaled_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection and Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Selection is deciding which features will be used in model\n",
    "Feature Engineering is creating new features that will help impove our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the unnecessary features\n",
    "telco = telco.drop(['Area_Code','Phone'],axis=1)\n",
    "\n",
    "# Verify dropped features\n",
    "print(telco.columns)\n",
    "\n",
    "Here, axis=1 indicates that you want to drop 'Area_Code' and 'Phone' from the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new feature that contains information about the average length of night calls made by customers.\n",
    "telco['Avg_Night_Calls'] = telco['Night_Mins']/telco['Night_Calls']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Churn Prediction by Supervised Learning - Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### by SVM;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC()\n",
    "svc.fit(telco['data'], telco['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!! Both data and target must be Pandas DataFrame or Numpy Array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### by LinearRegression;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Instantiate the classifier\n",
    "clf = LogisticRegression()\n",
    "\n",
    "# Fit the classifier\n",
    "# The features are contained in the features variable, and the target variable of interest is 'Churn'.\n",
    "clf.fit(telco[features], telco['Churn'])\n",
    "\n",
    "# Predict the label of new_customer\n",
    "print(clf.predict(new_customer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### by DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Instantiate the classifier\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Fit the classifier\n",
    "# The features are contained in the features variable, and the target variable of interest is 'Churn'.\n",
    "clf.fit(telco[features], telco['Churn'])\n",
    "\n",
    "# Predict the label of new_customer\n",
    "print(clf.predict(new_customer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Evaluating Model Performance, now by RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating training and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Before you create any model, it is important to split your dataset into two: a training set which will be used to build your churn model, and a test set which will be used to validate your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create feature variable by dropping the target variable 'Churn'\n",
    "X = telco.drop('Churn', axis=1)\n",
    "\n",
    "# Create target variable\n",
    "y = telco['Churn']\n",
    "\n",
    "# Create training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# Make a prediction\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Controlling the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train)\n",
    "len(X_test)\n",
    "\n",
    "X_train.shape\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Having split our data into training and testing sets, we can now fit your model to the training data and then predict the labels of the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* we've used Logistic Regression and Decision Trees. Here, we'll use a RandomForestClassifier, which we can think of as an ensemble of Decision Trees that generally outperforms a single Decision Tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Instantiate the classifier\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "# Fit to the training data\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "# Compute accuracy\n",
    "print(clf.score(X_test, y_test))\n",
    "\n",
    "[0.93]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Metrics - CONFUSION MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRUE POSITIVE = ACTUAL IS CHURN PREDICTED AS CHURN\n",
    "TRUE NEGATIVE = ACTUAL IS NON-CHURN PREDICTED AS NON-CHURN\n",
    "FALSE POSITIVE = ACTUAL IS NON-CHURN PREDICTED AS CHURN\n",
    "FALSE NEGATIVE = ACTUAL IS CHURN PREDICTED AS NON-CHURN\n",
    "\n",
    "\n",
    "PRECISION = TP / (TP+FP)\n",
    "\n",
    "RECALL = TP / (TP+FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using scikit-learn's confusion_matrix() function, you can easily create your classifier's confusion matrix and gain a more nuanced understanding of its performance. It takes in two arguments: The actual labels of your test set - y_test - and your predicted labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted labels of your Random Forest classifier from the previous exercise are stored in y_pred and were computed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(confusion_matrix(X_test,y_pred))\n",
    "\n",
    "([842  13]\n",
    " [53  92])\n",
    "\n",
    "842 = TN\n",
    "92  = TP\n",
    "53  = FN\n",
    "13  = FP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Varying training set size to get rid of overfitting and underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models learn better when they have more training data. However, there's a risk that they overfit to the training data and don't generalize well to new dataIt is betWhen setting test_size=0.2 instead of 0.3, the new confusion matrix ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "([550  8]\n",
    " [38  71])\n",
    "\n",
    "This classifier is higher precision than the before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the labels of the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Import precision_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# Print the precision\n",
    "print(precision_score(y_test, y_pred))\n",
    "\n",
    "# Import recall_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# Print the recall\n",
    "print(recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC Curve and computing AUC score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we need to check or visualize the performance of the multi - class classification problem, we use AUC (Area Under The Curve) ROC (Receiver Operating Characteristics) curve. It is one of the most important evaluation metrics for checking any classification modelâ€™s performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the probabilities\n",
    "y_pred_prob = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Import roc_curve\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Calculate the roc metrics\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "# Plot the ROC curve, \n",
    "# fpr = false positive rate, tpr = true positive rate\n",
    "plt.plot(fpr, tpr)\n",
    "\n",
    "# Add labels and diagonal line\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.plot([0, 1], [0, 1], \"k--\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import roc_auc_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Print the AUC\n",
    "print(roc_auc_score(y_test, y_pred_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision-Recall Curve - F1 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to evaluate model performance is using a precision-recall curve, which shows the tradeoff between precision and recall for different thresholds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC is one metric you can use in quantify model performance, and another is the F1 score, which is calculated as below:\n",
    "\n",
    "2 * (precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "! high F1 score is a sign of a well-performing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import f1_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Print the F1 score\n",
    "print(f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Tuning The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each model has hyperparameters.The default hyperparameters used by your models are not optimized for your data. We can modify them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the number of features with GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of grid search cross-validation is to identify those hyperparameters that lead to optimal model performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The n_estimators hyperparameter controls the number of trees to use in the forest, while the max_features hyperparameter controls the number features the random forest should consider when looking for the best split at decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create the hyperparameter grid\n",
    "param_grid = {'max_features': ['auto', 'sqrt', 'log2']}\n",
    "\n",
    "# Call GridSearchCV\n",
    "grid_search = GridSearchCV(clf, param_grid)\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print the optimal parameters\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "{'max_features': 'log2'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!!! It looks like taking a log of the number of features leads to optimal model performance. By default, the model takes the square root of the number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning multiple hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The power of GridSearchCV really comes into play when you're tuning multiple hyperparameters, as then the algorithm tries out all possible combinations of hyperparameters to identify the best combination. Here, you'll tune the following random forest hyperparameters:\n",
    "\n",
    "\n",
    "\n",
    "Hyperparameter\t    Purpose\n",
    "\n",
    "criterion\t   :    Quality of Split\n",
    "max_features   :\tNumber of features for best split\n",
    "max_depth\t   :    Max depth of tree\n",
    "bootstrap\t   :    Whether Bootstrap samples are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create the hyperparameter grid\n",
    "param_grid = {\"max_depth\": [3, None],\n",
    "              \"max_features\": [1, 3, 10],\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# Call GridSearchCV\n",
    "grid_search = GridSearchCV(clf, param_grid)\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X,y)\n",
    "\n",
    "# Print the optimal parameters\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning with Randomized Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** As the hyperparameter grid gets larger, grid search becomes slower. In order to solve this problem, instead of trying out every single combination of values, we could randomly jump around the grid and try different combinations. \n",
    "In scikit-learn, you can do this using RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!! look at the max_features below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import RandomizedSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Create the hyperparameter grid\n",
    "param_dist = {\"max_depth\": [3, None],\n",
    "              \"max_features\": randint(1, 11),\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# Call RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(clf, param_dist)\n",
    "\n",
    "# Fit the model\n",
    "random_search.fit(X,y)\n",
    "\n",
    "# Print best parameters\n",
    "print(random_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising Feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate feature importances\n",
    "importances = clf.feature_importances_\n",
    "\n",
    "# Create plot\n",
    "plt.barh(range(X.shape[1]), importances)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Improving the plot to understand better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make the plot more readable, we need to do achieve two goals:\n",
    "\n",
    "* Re-order the bars in ascending order.\n",
    "* Add labels to the plot that correspond to the feature names.\n",
    "\n",
    "The Numpy .argsort() method sorts an array and returns the indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort importances\n",
    "sorted_index = np.argsort(importances)\n",
    "\n",
    "# Create labels\n",
    "labels = X.columns[sorted_index]\n",
    "\n",
    "# Clear current plot\n",
    "plt.clf()\n",
    "\n",
    "# Create plot\n",
    "plt.barh(range(X.shape[1]), importances[sorted_index], tick_label=labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The plot tells us that CustServ_Calls, Day_Mins and Day_Charge are the most important drivers of churn. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
